name: NoTraM parliamentary speechs evaluation on v2 training checkpoints with 10 runs each
project: notram-v2_evals-speech-10runs
program: notram_eval.py
entity: nbailab
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - ${args}
method: grid
metric:
  name: eval:f1_macro
  goal: maximize
parameters:
  dataset_name:
    value: NbAiLab/norwegian_parliament
  task_name:
    value: "speech"
  model_name:
    values:
    - ./v2_evals/checkpoints/T8POD_BERT_base_scandinavian_uncased_380k
    - ./v2_evals/checkpoints/T6_noTram2_BERT_norwegian_cased_3.8m
    - ./v2_evals/checkpoints/T7_noTram2_BERT_norwegian_uncased_3.8m
    - ./v2_evals/checkpoints/T10_BERT_large_norwegian_uncased
    - ./v2_evals/checkpoints/T6POD_BERT_base_norwegian_cased_decay
    - ./v2_evals/checkpoints/T7POD_BERT_base_norwegian_uncased_decay_2021-03-28_19-36-06
    - NbAiLab/nb-bert-base
    - bert-base-multilingual-cased
    - ltgoslo/norbert
  force_download:
    value: true
  num_train_epochs:
    value: 3
  warmup_steps:
    value: 0.1
  weight_decay:
    value: 0.0
  learning_rate:
    value: 3e-5
  train_batch_size:
    value: 8
  cache_dir:
    value: ./cache
  output_dir:
    value: ./output
  run:
    values: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
